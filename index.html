<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance">
  <meta name="keywords" content="Robotics, Navigation, Spot, Computer Vision, RGB-only">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Last-meter Nav: Last-meter Navigation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://rpm-lab.github.io/">
      <span class="icon">
          <img src="./static/images/favicon.ico" alt="RPM Lab Icon">
      </span>
      <span style="margin-left: 5px;">Robotics: Perception and Manipulation (RPM) Lab</span>
      </a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://bentzuhsien.github.io/">Tzu-Hsien Lee</a>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/fidan-mahmudova/">Fidan Mahmudova</a>,</span>
            <span class="author-block">
              <a href="https://karthikdesingh.com/">Karthik Desingh</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Minnesota,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2512.11173"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.11173"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Last-meter navigation enables robots to achieve <strong>manipulation-ready positioning</strong>, bridging the critical gap between global navigation and manipulation.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      
      <div class="column">
        <span class="icon is-large">
            <i class="fas fa-route fa-3x"></i>
        </span>
        <h3 class="title is-4 mt-4">Last-Meter Navigation</h3>
        <p>
          Object-centric imitation learning framework that solves the last-meter navigation problem and produces manipulation-ready base poses with centimeter-level accuracy.
        </p>
      </div>

      <div class="column">
        <span class="icon is-large">
            <i class="fas fa-chair fa-3x"></i>
        </span>
        <h3 class="title is-4 mt-4">One-Instance Transfer</h3>
        <p>
          Demonstration of strong instance-to-category generalization, where a model trained on a single object instance reliably transfers to unseen objects of the same category.
        </p>
      </div>

      <div class="column">
        <span class="icon is-large">
            <i class="fas fa-eye fa-3x"></i>
        </span>
        <h3 class="title is-4 mt-4">RGB-Only Perception</h3>
        <p>
          Real-world validation that precise last-meter navigation is achievable using only onboard RGB observations, without depth, LiDAR, or map priors.
        </p>
      </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation.
          </p>
          <p>
            This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for <i>last-meter navigation</i>, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras.
          </p>
          <p>
            Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves <strong>73.47%</strong> success in edge-alignment and <strong>96.94%</strong> success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Sequential Last-meter Navigation for Mobile Manipulation</h2>
        <div class="content has-text-justified">
          <p>
            Last-meter navigation enables the robot to navigate effectively between different objects, facilitating sequential multi-stage mobile manipulation tasks. By chaining last-meter navigation policies, the robot can transition from one workspace to another with high precision.
          </p>
        </div>
        <div class="publication-video">
          <video id="sequential-nav" autoplay controls loop playsinline height="100%">
            <source src="./static/videos/Moving_Between_Objects.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Last-meter Navigation</h2>
    <div class="columns is-centered is-vcentered">
      
      <div class="column is-6">
        <figure class="image">
          <video
            src="./static/videos/last-meter_navigation.mp4"
            autoplay
            loop
            muted
            playsinline
            style="max-width: 100%; height: auto;">
          </video>
        </figure>
      </div>

      <div class="column is-6">
        <div class="content has-text-justified">
          <p>
            Last-meter navigation is the stage between global path planning and manipulation in which the robot must achieve centimeter-level positional and degree-level orientation accuracy relative to a target. Whereas global navigation often deems success as stopping within about one meter, manipulation policies operate reliably only under much tighter alignment, and this mismatch causes many mobile manipulation failures. Last-meter navigation addresses this gap by explicitly focusing on the final meter of motion so that the robot arrives in a manipulation-ready pose.
          </p>
          <p>
            In the example on the left: (1) global navigation first drives the robot near the target; (2) once the target object (e.g., the orange chair) is detected, our policy is invoked; and (3) last-meter navigation adjusts the robotâ€™s base to a precise manipulation-ready pose defined by a goal observation.
          </p>
        </div>

    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Robustness & Generalization</h2>
    <div class="content has-text-justified">
      <p>
        Our method shows strong instance-to-category generalization, operating successfully with unseen objects and in novel indoor/outdoor environments.
      </p>
    </div>
    <div class="columns is-centered">
      <div class="column is-one-third">
        <h5 class="title is-6 has-text-centered">Indoor Env 1</h5>
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/indoor1.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-third">
        <h5 class="title is-6 has-text-centered">Indoor Env 2</h5>
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/indoor2.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column is-one-third">
        <h5 class="title is-6 has-text-centered">Outdoor Env</h5>
        <video autoplay controls muted loop playsinline width="100%">
          <source src="./static/videos/outdoor.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Methodology</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image">
            <img src="./static/images/methodology.png" alt="System Architecture">
        </figure>
        <div class="content has-text-justified mt-4">
          <p>
            <strong>Architecture Overview.</strong> At each timestep, the model receives current and goal observations. A segmentation module (driven by a language prompt) generates object masks. The action decoder uses a spatial score-matrix to predict discrete actions (Forward, Lateral, Rotate).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="qualitative-results">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Qualitative Results</h2>

    <div class="content has-text-justified mb-4">
      <p>
        We present qualitative results of category-level last-meter navigation across a diverse set of target objects and environments.
        Each scenario corresponds to a distinct object instance and scene configuration.
        Multiple trials are shown for each scenario to highlight consistency, robustness, and failure modes under varying initial conditions.
      </p>
      <p>
        Use the menu below to select a scenario and a corresponding trial:
      </p>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-centered">
          <div class="select is-medium mr-2">
            <select id="scenario-menu" onchange="updateTrialOptions(); updateQualVideo();">
              <option value="Scenario1" selected>Scenario1</option>
              <option value="Scenario2">Scenario2</option>
              <option value="Scenario3">Scenario3</option>
              <option value="Scenario4">Scenario4</option>
              <option value="Scenario5">Scenario5</option>
              <option value="Scenario6">Scenario6</option>
              <option value="Scenario7">Scenario7</option>
              <option value="Scenario8">Scenario8</option>
              <option value="Scenario9">Scenario9</option>
              <option value="Scenario10">Scenario10</option>
            </select>
          </div>

          <div class="select is-medium">
            <select id="trial-menu" onchange="updateQualVideo();">
              <!-- options will be filled by JS -->
            </select>
          </div>
        </div>

        <div class="publication-video mt-4">
          <video id="qual-video" controls autoplay loop muted playsinline width="100%">
            <source id="qual-video-src" src="" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        
        <figure class="image mb-5">
            <img src="./static/images/part1.png" alt="Part 1 Results">
        </figure>
        
        <figure class="image">
            <img src="./static/images/part2.png" alt="Part 2 Results">
        </figure>

        <div class="content has-text-justified mt-4">
          <p>
             Our system (DinoScoreAux) achieves the highest success rate among baselines, demonstrating the importance of the spatial score-matrix decoder and auxiliary stopping mechanism.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{lee2025learning,
  title={Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance},
  author={Lee, Tzu-Hsien and Mahmudova, Fidan and Desingh, Karthik},
  journal={arXiv preprint arXiv:2512.11173},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  // Map: scenario -> array of available trials
  const SCENARIO_TO_TRIALS = {
    "Scenario1": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario2": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario3": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario4": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario5": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario6": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario7": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario8": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario9": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
    "Scenario10": ["trial1", "trial2", "trial3", "trial4", "trial5", "trial6", "trial7", "trial8", "trial9", "trial10"],
  };

  const VIDEO_BASE = "./static/videos/qualitative";

  function buildVideoPath(scenario, trial) {
    return `${VIDEO_BASE}/${scenario}/${trial}.mp4`;
  }

  function updateTrialOptions() {
    const scenario = document.getElementById("scenario-menu").value;
    const trialMenu = document.getElementById("trial-menu");

    const trials = SCENARIO_TO_TRIALS[scenario] || ["trial1"];
    trialMenu.innerHTML = "";

    for (const t of trials) {
      const opt = document.createElement("option");
      opt.value = t;
      opt.textContent = t;
      trialMenu.appendChild(opt);
    }
  }

  function updateQualVideo() {
    const scenario = document.getElementById("scenario-menu").value;
    const trial = document.getElementById("trial-menu").value;

    const video = document.getElementById("qual-video");
    const source = document.getElementById("qual-video-src");

    source.src = buildVideoPath(scenario, trial);
    video.load();
  }

  window.addEventListener("DOMContentLoaded", () => {
    updateTrialOptions();
    updateQualVideo();
  });
</script>

</body>
</html>